{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning Model Comparison\n",
    "\n",
    "Recall one formulation of the data science process.\n",
    "\n",
    "1. Define the problem.\n",
    "2. Gather the data.\n",
    "3. Explore the data.\n",
    "4. Model the data.\n",
    "5. Evaluate the model.\n",
    "6. Answer the problem.\n",
    "\n",
    "In this lab, we're going to focus mostly on creating (and then comparing) many regression and classification models. We'll define the problem and gather the data for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define the problem.\n",
    "\n",
    "You are a data scientist with a financial services company. Specifically, you want to leverage data to identify potential customers.\n",
    "\n",
    "If you are unfamiliar with \"401(k)s\" or \"IRAs,\" these are two types of retirement accounts. Very broadly speaking:\n",
    "- You can put money for retirement into both of these accounts.\n",
    "- The money in these accounts gets invested and hopefully has a lot more money in it when you retire.\n",
    "- These are a little different from regular bank accounts in that they have tax benefits. Also, employers frequently match money that you put into a 401k.\n",
    "- If you want to learn more about them, you check out [this site](investopedia.com/ask/answers/12/401k.asp).\n",
    "\n",
    "We will tackle one regression problem and one classification problem today.\n",
    "- Regression: What features best predict one's income?\n",
    "- Classification: Predict whether someone is eligible for a 401k.\n",
    "\n",
    "Check out the data dictionary [here](http://fmwww.bc.edu/ec-p/data/wooldridge2k/401KSUBS.DES).\n",
    "\n",
    "---\n",
    "### NOTE ⚠️\n",
    "\n",
    "When predicting `inc`, you should pretend as though you do not have access to the `e401k`, the `p401k` variable, and the `pira` variable. When predicting `e401k`, you may use the entire dataframe if you wish.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, confusion_matrix, plot_confusion_matrix, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Import the data.\n",
    "\n",
    "##### 1. Read in the data from the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e401k</th>\n",
       "      <th>inc</th>\n",
       "      <th>marr</th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>fsize</th>\n",
       "      <th>nettfa</th>\n",
       "      <th>p401k</th>\n",
       "      <th>pira</th>\n",
       "      <th>incsq</th>\n",
       "      <th>agesq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>13.170</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>4.575</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>173.4489</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>61.230</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>154.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3749.1130</td>\n",
       "      <td>1225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>12.858</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>165.3282</td>\n",
       "      <td>1936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>98.880</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>21.800</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9777.2540</td>\n",
       "      <td>1936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>22.614</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>18.450</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>511.3930</td>\n",
       "      <td>2809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9270</th>\n",
       "      <td>0</td>\n",
       "      <td>58.428</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3413.8310</td>\n",
       "      <td>1089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9271</th>\n",
       "      <td>0</td>\n",
       "      <td>24.546</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>3</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>602.5061</td>\n",
       "      <td>1369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9272</th>\n",
       "      <td>0</td>\n",
       "      <td>38.550</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>3</td>\n",
       "      <td>-13.600</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1486.1020</td>\n",
       "      <td>1089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9273</th>\n",
       "      <td>0</td>\n",
       "      <td>34.410</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>3</td>\n",
       "      <td>3.550</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1184.0480</td>\n",
       "      <td>3249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9274</th>\n",
       "      <td>0</td>\n",
       "      <td>25.608</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>1.800</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>655.7697</td>\n",
       "      <td>2401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9275 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      e401k     inc  marr  male  age  fsize   nettfa  p401k  pira      incsq  \\\n",
       "0         0  13.170     0     0   40      1    4.575      0     1   173.4489   \n",
       "1         1  61.230     0     1   35      1  154.000      1     0  3749.1130   \n",
       "2         0  12.858     1     0   44      2    0.000      0     0   165.3282   \n",
       "3         0  98.880     1     1   44      2   21.800      0     0  9777.2540   \n",
       "4         0  22.614     0     0   53      1   18.450      0     0   511.3930   \n",
       "...     ...     ...   ...   ...  ...    ...      ...    ...   ...        ...   \n",
       "9270      0  58.428     1     0   33      4   -1.200      0     0  3413.8310   \n",
       "9271      0  24.546     0     1   37      3    2.000      0     0   602.5061   \n",
       "9272      0  38.550     1     0   33      3  -13.600      0     1  1486.1020   \n",
       "9273      0  34.410     1     0   57      3    3.550      0     0  1184.0480   \n",
       "9274      0  25.608     0     1   49      1    1.800      0     0   655.7697   \n",
       "\n",
       "      agesq  \n",
       "0      1600  \n",
       "1      1225  \n",
       "2      1936  \n",
       "3      1936  \n",
       "4      2809  \n",
       "...     ...  \n",
       "9270   1089  \n",
       "9271   1369  \n",
       "9272   1089  \n",
       "9273   3249  \n",
       "9274   2401  \n",
       "\n",
       "[9275 rows x 11 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers = pd.read_csv('401ksubs.csv')\n",
    "customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. What are 2-3 other variables that, if they were available, would be helpful to have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Education level, location, job title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Suppose a peer recommended putting `race` into your model to better predict who to target when advertising IRAs and 401(k)s. Why might this be unethical?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might be unethical because race is private information to the customer and it should not be used in an analysis that has a lot of eyes on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Explore the data.\n",
    "\n",
    "##### 4. When attempting to predict income, which feature(s) would we exclude? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9275 entries, 0 to 9274\n",
      "Data columns (total 11 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   e401k   9275 non-null   int64  \n",
      " 1   inc     9275 non-null   float64\n",
      " 2   marr    9275 non-null   int64  \n",
      " 3   male    9275 non-null   int64  \n",
      " 4   age     9275 non-null   int64  \n",
      " 5   fsize   9275 non-null   int64  \n",
      " 6   nettfa  9275 non-null   float64\n",
      " 7   p401k   9275 non-null   int64  \n",
      " 8   pira    9275 non-null   int64  \n",
      " 9   incsq   9275 non-null   float64\n",
      " 10  agesq   9275 non-null   int64  \n",
      "dtypes: float64(3), int64(8)\n",
      "memory usage: 797.2 KB\n"
     ]
    }
   ],
   "source": [
    "customers.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other than the variables listed above (the e401k, the p401k, and the pira variables), when predicting the inc variable we should exclude incsq because it has been created based off of our y variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. What two variables have already been created for us through feature engineering? Come up with a hypothesis as to why subject-matter experts may have done this.\n",
    "> This need not be a \"statistical hypothesis.\" Just brainstorm why SMEs might have done this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Income squared and age squared have been added to the data. If these variables don't have a linear relationship with the independent variable, the squared values will allow the model to more accurately capture the effects of income and age."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Looking at the data dictionary, one or more variable descriptions appear to be erroneous. What's the issue and what do you think the correct value(s) should be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Income is described as inc^2 and age is described as age^2. I think they are both intended to show as the original income level and age of the customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Model the data. (Part 1: Regression Problem)\n",
    "\n",
    "- Problem: What features best predict one's income?\n",
    "- When predicting `inc`, you should pretend as though you do not have access to the `e401k`, the `p401k` variable, and the `pira` variable.\n",
    "\n",
    "##### 7. List all models you've learned through the date this lab was assigned that could be used to solve a regression problem. For each model type, identify whether it might be appropriate for solving this specific regression problem and explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* linear regression - yes, the data is continous and may be linear\n",
    "* ridge - yes, is a form of linear\n",
    "* lasso - yes, is a form of linear\n",
    "* elastic net - yes, is a form of linear\n",
    "* knn - not sure, may not produce the most accurate results on a continuous dataset\n",
    "* decision trees - yes, this data can be split based on lowest mse\n",
    "* random forests - yes, related to decision trees\n",
    "* bagging models - yes, related to decision trees\n",
    "* boosting models - yes, related to decision trees\n",
    "* ann - no, may be overcomplicated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8. Regardless of your answer to number 7, fit at least one of each of the following models to attempt to solve the regression problem above:\n",
    "    - a multiple linear regression model\n",
    "    - a K-nearest neighbors model\n",
    "    - a decision tree\n",
    "    - a set of bagged decision trees\n",
    "    - a random forest\n",
    "    - a boosting model\n",
    "\n",
    "    \n",
    "> As always, be sure to do a train/test split! To compare modeling techniques, use the same train-test split on each. \n",
    "\n",
    "> You may find it helpful to set up pipelines, but you are not required to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = customers.drop(columns = ['e401k', 'p401k', 'pira', 'inc', 'incsq'])\n",
    "y = customers['inc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 88)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2788276937242058"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31708172488564146"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42793521708417304"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = DecisionTreeRegressor(max_depth = 6)\n",
    "tree.fit(X_train, y_train)\n",
    "tree.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41567052241802216"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44844326533841894"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest = RandomForestRegressor(n_estimators = 300, max_depth = 6)\n",
    "forest.fit(X_train, y_train)\n",
    "forest.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4347062804270504"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9. What is bootstrapping?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Random resampling with replacement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10. What is the difference between a decision tree and a set of bagged decision trees? Be specific!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree is one instance of the model and tends to be overfit to the training set, but a set of bagged decision trees take multiple different sub-samples of the data and aggregates the individual decision trees created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 11. What is the difference between a set of bagged decision trees and a random forest? Be specific!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest uses a modified tree learning algorithm that selects, at each split in the learning process, a random subset of the features. Bagged decision trees will repeatedly use the same set of features, which causes them to all be correlated and creates high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 12. Why might a random forest be superior to a set of bagged decision trees?\n",
    "> Hint: Consider the bias-variance tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, bagging usually has high variance because it's using the same samples of features. Using random forest lowers the variance, but could increase bias as a result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Evaluate the model. (Part 1: Regression Problem)\n",
    "\n",
    "##### 13. Using RMSE, evaluate each of the models you fit on both the training and testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_preds_train = lr.predict(X_train)\n",
    "lr_preds_test = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.389791684422967"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_train, lr_preds_train, squared = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.09565239559958"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, lr_preds_test, squared = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_preds_train = tree.predict(X_train)\n",
    "tree_preds_test = tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.16000127426358"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_train, tree_preds_train, squared = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.58859821305125"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, tree_preds_test, squared = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_preds_train = forest.predict(X_train)\n",
    "forest_preds_test = forest.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.83151996432132"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_train, forest_preds_train, squared = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.283309993287876"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, forest_preds_test, squared = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.31743381631067"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#null model\n",
    "null_pred = np.full_like(y_test, np.mean(y_test))\n",
    "mean_squared_error(y_test, null_pred, squared = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 14.Which model performs best?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest regressor performs the best on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 15. Based on everything we've covered so far, if you had to pick just one model as your final model to use for this problem, which one model would you pick? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the information covered in this lab I would pick the random forest regressor because it has the lowest mse and I think it would be the most reliable model if it was introduced with new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 16. Suppose you wanted to improve the performance of your final model. Brainstorm 2-3 things that, if you had more time, you would attempt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would improve my model by:\n",
    "* trying different parameters like more estimators or adjust max_depth\n",
    "* scaling the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Model the data. (Part 2: Classification Problem)\n",
    "\n",
    "- Problem: Predict whether or not one is eligible for a 401k.\n",
    "- When predicting `e401k`, you may use the entire dataframe if you wish.\n",
    "\n",
    "##### 17. While you're allowed to use every variable in your dataframe, mention at least one disadvantage of using `p401k` in your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will become overfit to the training data because if someone is participating in the 401k plan, it means they are eligible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 18. List all models you've learned that could be used to solve a classification problem. For each, identify whether it is appropriate for solving this specific classification problem and explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* logistic regression - yes, the data is discrete and may be linear\n",
    "* knn - yes, the probability can be found based on similarities to other data points\n",
    "* decision trees - yes, this data can be split based on Gini impurity\n",
    "* random forests - yes, related to decision trees\n",
    "* bagging models - yes, related to decision trees\n",
    "* boosting models - yes, related to decision trees\n",
    "* ann - no, may be overcomplicated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 19. Regardless of your answer to number 18, fit at least one of each of the following models to attempt to solve the classification problem above:\n",
    "    - a logistic regression model\n",
    "    - a K-nearest neighbors model\n",
    "    - a decision tree\n",
    "    - a set of bagged decision trees\n",
    "    - a random forest\n",
    "    - a boosting model\n",
    "    \n",
    "> As always, be sure to do a train/test split! In order to compare modeling techniques, you should use the same train-test split on each. I recommend using a random seed here.\n",
    "\n",
    "> You may find it helpful to set up a pipeline to try each modeling technique, but you are not required to do so!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = customers.drop(columns = 'e401k')\n",
    "y2 = customers['e401k']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, random_state = 88)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8842725704427832"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logr = LogisticRegression(max_iter = 1000)\n",
    "logr.fit(X2_train, y2_train)\n",
    "logr.score(X2_train, y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8835705045278137"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logr.score(X2_test, y2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7390741805635422"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X2_train, y2_train)\n",
    "knn.score(X2_train, y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.605002156101768"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.score(X2_test, y2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rforest = RandomForestClassifier()\n",
    "rforest.fit(X2_train, y2_train)\n",
    "rforest.score(X2_train, y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8831392841742131"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rforest.score(X2_test, y2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.598534\n",
       "1    0.401466\n",
       "Name: e401k, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#null model\n",
    "y2_test.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Evaluate the model. (Part 2: Classfication Problem)\n",
    "\n",
    "##### 20. Suppose our \"positive\" class is that someone is eligible for a 401(k). What are our false positives? What are our false negatives?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr_preds_train = logr.predict(X2_train)\n",
    "logr_preds_test = logr.predict(X2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1388,    0],\n",
       "       [ 270,  661]], dtype=int64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_logr = confusion_matrix(y2_test, logr_preds_test)\n",
    "cm_logr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'False Positives: 0, False Negatives: 270'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn, fp, fn, tp = cm_logr.ravel()\n",
    "f'False Positives: {fp}, False Negatives: {fn}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_preds_train = knn.predict(X2_train)\n",
    "knn_preds_test = knn.predict(X2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1004,  384],\n",
       "       [ 532,  399]], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_knn = confusion_matrix(y2_test, knn_preds_test)\n",
    "cm_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'False Positives: 384, False Negatives: 532'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn, fp, fn, tp = cm_knn.ravel()\n",
    "f'False Positives: {fp}, False Negatives: {fn}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "rforest_preds_train = rforest.predict(X2_train)\n",
    "rforest_preds_test = rforest.predict(X2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1376,   12],\n",
       "       [ 259,  672]], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_rforest = confusion_matrix(y2_test, rforest_preds_test)\n",
    "cm_rforest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'False Positives: 12, False Negatives: 259'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn, fp, fn, tp = cm_rforest.ravel()\n",
    "f'False Positives: {fp}, False Negatives: {fn}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 21. In this specific case, would we rather minimize false positives or minimize false negatives? Defend your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should minimize false negatives. Because this isn't a life or death situation, I think the goal would be to not miss anyone who is eligible. It's more important for a financial services company to accidentally send marketing to someone who isn't eligible than to miss out on a potential customer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 22. Suppose we wanted to optimize for the answer you provided in problem 21. Which metric would we optimize in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 23. Suppose that instead of optimizing for the metric in problem 21, we wanted to balance our false positives and false negatives using `f1-score`. Why might [f1-score](https://en.wikipedia.org/wiki/F1_score) be an appropriate metric to use here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The f1-score is an appropriate metric because there is no right or wrong answer to the question above - all metrics are useful depending on how you look at the situation. f1-score gives a good overall performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 24. Using f1-score, evaluate each of the models you fit on both the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Train: 0.8252658997178206, Test: 0.8304020100502513'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'Train: {f1_score(y2_train, logr_preds_train)}, Test: {f1_score(y2_test, logr_preds_test)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Train: 0.638085742771685, Test: 0.46557759626604434'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'Train: {f1_score(y2_train, knn_preds_train)}, Test: {f1_score(y2_test, knn_preds_test)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Train: 1.0, Test: 0.8321981424148606'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'Train: {f1_score(y2_train, rforest_preds_train)}, Test: {f1_score(y2_test, rforest_preds_test)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 25. Based on training f1-score and testing f1-score, is there evidence of a great deal overfitting in any of your models? Which ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN and Random Forest are overfit compared to logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 26. Based on everything we've covered so far, if you had to pick just one model as your final model to use to answer the problem in front of you, which one model would you pick? Defend your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would pick Logistic Regression because it has the highest f1 score on the test set and is not overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27. Suppose you wanted to improve the performance of your final model. Brainstorm 2-3 things that, if you had more time, you would attempt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would improve my model by:\n",
    "* trying different parameters on Logistic Regression for the penalty or C\n",
    "* scaling the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Answer the problem.\n",
    "\n",
    "##### Briefly summarize your answers to the regression and classification problems. Be sure to include any limitations or hesitations in your answer.\n",
    "\n",
    "- Regression: What features best predict one's income?\n",
    "- Classification: Predict whether or not one is eligible for a 401k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.sort_values of                0\n",
       "marr    0.221671\n",
       "male    0.011232\n",
       "age     0.033686\n",
       "fsize   0.012849\n",
       "nettfa  0.684532\n",
       "agesq   0.036030>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importances = forest.feature_importances_\n",
    "features = X_train.columns\n",
    "pd.DataFrame(importances, features).sort_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose random forest as my final regression model. The model's R2 is 43.5% on my test set and it's RMSE is 18.27, which lower than the baseline model's at 24.31. It scored higher than other models I tried, but could probably be improved by changing some of the parameters. I left everything as the default, but if there was more time would do a grid search to explore more option. Marriage and sex are the best predictors of one's income. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose logsistic regression as my classification model. The accuracy score was 88.4% on the test set, which is better than all other models and the baseline of 59%. Even though this model didn't have the highest f1 score of all the models I tested it was the most consistent across the train and test sets. Again, if there was more time I would use grid search to test multiple parameters to find the best fit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
